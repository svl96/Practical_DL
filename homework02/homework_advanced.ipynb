{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NqtXrZApBkM4"
   },
   "source": [
    "# Optimizing training and inference\n",
    "\n",
    "In this notebook, we will discuss different ways to reduce memory and compute usage during training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NEt8wg4JCQdm"
   },
   "source": [
    "## Prepare training script (1 point)\n",
    "\n",
    "When training large models, it is usually a best practice not to use Jupyter notebooks, but run a **separate script** for training which could have command-line flags for various hyperparameters and training modes. This is especially useful when you need to run multiple experiments simultaneously (e.g. on a cluster with task scheduler). Another advantage of this is that after training, the process will finish and free the resources for other users of a shared GPU.\n",
    "\n",
    "In this part, you will need to put all your code to train a model on Tiny ImageNet that you wrote for the previous task in `train.py`.\n",
    "\n",
    "You can then run your script from inside of this notebook like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --epochs=2 --batch=200 --batch_count=5 --memory_usage --plot_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DONE! \n",
    "\n",
    "Whole lot of code goes to files\n",
    "- train.py\n",
    "- utils.py\n",
    "- cls_models.py\n",
    "- dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** \n",
    "\n",
    "Write code for training with architecture from homework_part2\n",
    "\n",
    "**Requirements**\n",
    "* Optional arguments from command line such as batch size and number of epochs with built-in argparse\n",
    "* Modular structure - separate functions for creating data generator, building model and training \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKPYZ3QLEqX8"
   },
   "source": [
    "## Profiling time (1 point)\n",
    "\n",
    "For the next tasks, you need to add measurements to your training loop. You can use [`perf_counter`](https://docs.python.org/3/library/time.html#time.perf_counter) for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication took 0.052 seconds\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(1000, 1000)\n",
    "y = np.random.randn(1000, 1000)\n",
    "\n",
    "start_counter = time.perf_counter()\n",
    "z = x @ y\n",
    "elapsed_time = time.perf_counter() - start_counter\n",
    "print(\"Matrix multiplication took {:.3f} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfhLeWjTGTpB"
   },
   "source": [
    "**Task**. You need to add the following measurements to your training script:\n",
    "* How much time a forward-backward pass takes for a single batch;\n",
    "* How much time an epoch takes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE! \n",
    "\n",
    "Create a useful class Profiler - for profiling time of different tasks\n",
    "\n",
    "At cell below example of program logging \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Logging Timestamp 2020-03-09 13.00.28\n\nNamespace(batch=200, batch_count=1, checkpoint=-1, epochs=2, memory_usage=False, model='standard', num_workers=4, opt_lr=0.001, out='saved_models/model_standard_2020-03-09 13.00.28.txt', plot_hist=False, pretrained=False, save_best=True, stop_accuracy=0.4, teacher_file='')\ndevice cuda:0\nBatch size 200\nRun training\nEpoch [1/2] Time: 71.07s; BatchTime:0.15s; Loss: 4.2169; Accuracy: 0.1175; ValLoss: 3.8208; ValAccuracy: 0.1678\nEpoch [2/2] Time: 70.93s; BatchTime:0.15s; Loss: 3.3600; Accuracy: 0.2499; ValLoss: 3.2819; ValAccuracy: 0.2638\n\nTEST Loss: 3.2794; Accuracy: 0.2632\n\n\n"
    }
   ],
   "source": [
    "!python train.py --epochs=2 --batch=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "khDOTn_SHaND"
   },
   "source": [
    "## Profiling memory usage (1 point)\n",
    "\n",
    "**Task**. You need to measure the memory consumptions\n",
    "\n",
    "This section depends on whether you train on CPU or GPU.\n",
    "\n",
    "### If you train on CPU\n",
    "You can use GNU time to measure peak RAM usage of a script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98xvXSjUIDzl"
   },
   "outputs": [],
   "source": [
    "!/usr/bin/time -lp python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1ES2Pc9IlH5"
   },
   "source": [
    "**Maximum resident set size**  will show you the peak RAM usage in bytes after the script finishes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**. \n",
    "Imports also require memory, do the correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kq5lY5CKJHX1"
   },
   "source": [
    "### If you train on GPU\n",
    "\n",
    "Use [`torch.cuda.max_memory_allocated()`](https://pytorch.org/docs/stable/cuda.html#torch.cuda.max_memory_allocated) at the end of your script to show the maximum amount of memory in bytes used by all tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE! \n",
    "To enable displaying `memory usage` you should add flag `--memory_usage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Logging Timestamp 2020-03-09 13.06.23\n\nNamespace(batch=200, batch_count=1, checkpoint=-1, epochs=2, memory_usage=True, model='standard', num_workers=4, opt_lr=0.001, out='saved_models/model_standard_2020-03-09 13.06.23.pth', plot_hist=False, pretrained=False, save_best=True, stop_accuracy=0.4, teacher_file='')\ndevice cuda:0\nBatch size 200\nRun training\nEpoch [1/2] Time: 71.54s; BatchTime:0.15s; Loss: 4.2160; Accuracy: 0.1175; ValLoss: 3.7632; ValAccuracy: 0.1828\nEpoch [2/2] Time: 71.08s; BatchTime:0.15s; Loss: 3.3775; Accuracy: 0.2438; ValLoss: 3.2805; ValAccuracy: 0.2594\n\nTEST Loss: 3.2741; Accuracy: 0.2609\n\nPeak memory usage by Pytorch tensors: 1500.44 Mb\n\n"
    }
   ],
   "source": [
    "!python train.py --epochs=2 --batch=200 --memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3RWHxYKBUys"
   },
   "source": [
    "## Gradient based techniques\n",
    "\n",
    "Modern architectures can potentially consume lots and lots of memory even for minibatch of several objects. To handle such cases here we will discuss two simple techniques.\n",
    "\n",
    "### Gradient Checkpointing (3 points)\n",
    "\n",
    "Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model.\n",
    "\n",
    "See [blogpost](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9) for kind introduction and different strategies or [article](https://arxiv.org/pdf/1604.06174.pdf) for not kind introduction.\n",
    "\n",
    "**Task**. Use [built-in checkpointing](https://pytorch.org/docs/stable/checkpoint.html), measure the difference in memory/compute \n",
    "\n",
    "**Requirements**. \n",
    "* Try several arrangements for checkpoints\n",
    "* Add the chekpointing as the optional flag into your script\n",
    "* Measure the difference in memory/compute between the different arrangements and baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Logging Timestamp 2020-03-09 13.12.36\n\nNamespace(batch=200, batch_count=1, checkpoint=-1, epochs=15, memory_usage=True, model='standard', num_workers=4, opt_lr=0.001, out='saved_models/model_standard_2020-03-09 13.12.36.pth', plot_hist=False, pretrained=False, save_best=True, stop_accuracy=0.4, teacher_file='')\ndevice cuda:0\nBatch size 200\nRun training\nEpoch [1/15] Time: 71.77s; BatchTime:0.15s; Loss: 4.2220; Accuracy: 0.1168; ValLoss: 3.7475; ValAccuracy: 0.1805\nEpoch [2/15] Time: 70.86s; BatchTime:0.15s; Loss: 3.3761; Accuracy: 0.2452; ValLoss: 3.2957; ValAccuracy: 0.2615\nEpoch [3/15] Time: 70.94s; BatchTime:0.15s; Loss: 2.9508; Accuracy: 0.3261; ValLoss: 2.9471; ValAccuracy: 0.3269\nEpoch [4/15] Time: 70.85s; BatchTime:0.15s; Loss: 2.6851; Accuracy: 0.3722; ValLoss: 2.8379; ValAccuracy: 0.3486\nEpoch [5/15] Time: 70.71s; BatchTime:0.15s; Loss: 2.4874; Accuracy: 0.4132; ValLoss: 2.7098; ValAccuracy: 0.3795\nEpoch [6/15] Time: 71.03s; BatchTime:0.15s; Loss: 2.3320; Accuracy: 0.4427; ValLoss: 2.6526; ValAccuracy: 0.3918\nEpoch [7/15] Time: 70.76s; BatchTime:0.15s; Loss: 2.2079; Accuracy: 0.4657; ValLoss: 2.5978; ValAccuracy: 0.4030\nEpoch [8/15] Time: 70.74s; BatchTime:0.15s; Loss: 2.0982; Accuracy: 0.4899; ValLoss: 2.6052; ValAccuracy: 0.4076\n\nTEST Loss: 2.6012; Accuracy: 0.4108\n\nPeak memory usage by Pytorch tensors: 1500.44 Mb\n\n"
    }
   ],
   "source": [
    "#baseline \n",
    "!python train.py --epochs=15 --batch=200 --memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Logging Timestamp 2020-03-09 13.40.47\n\nNamespace(batch=600, batch_count=1, checkpoint=-1, epochs=15, memory_usage=True, model='standard', num_workers=4, opt_lr=0.001, out='saved_models/model_standard_2020-03-09 13.40.47.pth', plot_hist=False, pretrained=False, save_best=True, stop_accuracy=0.4, teacher_file='')\ndevice cuda:0\nBatch size 600\nRun training\nEpoch [1/15] Time: 80.65s; BatchTime:0.51s; Loss: 4.3642; Accuracy: 0.0992; ValLoss: 4.0091; ValAccuracy: 0.1454\nEpoch [2/15] Time: 78.48s; BatchTime:0.51s; Loss: 3.6122; Accuracy: 0.2064; ValLoss: 3.5395; ValAccuracy: 0.2257\nEpoch [3/15] Time: 79.25s; BatchTime:0.50s; Loss: 3.1844; Accuracy: 0.2810; ValLoss: 3.2285; ValAccuracy: 0.2742\nEpoch [4/15] Time: 79.09s; BatchTime:0.50s; Loss: 2.8929; Accuracy: 0.3347; ValLoss: 2.9818; ValAccuracy: 0.3271\nEpoch [5/15] Time: 79.21s; BatchTime:0.50s; Loss: 2.6748; Accuracy: 0.3796; ValLoss: 2.8735; ValAccuracy: 0.3473\nEpoch [6/15] Time: 79.18s; BatchTime:0.50s; Loss: 2.5090; Accuracy: 0.4098; ValLoss: 2.7947; ValAccuracy: 0.3653\nEpoch [7/15] Time: 79.16s; BatchTime:0.50s; Loss: 2.3725; Accuracy: 0.4361; ValLoss: 2.6629; ValAccuracy: 0.3885\nEpoch [8/15] Time: 79.10s; BatchTime:0.50s; Loss: 2.2467; Accuracy: 0.4621; ValLoss: 2.8020; ValAccuracy: 0.3725\nEpoch [9/15] Time: 78.83s; BatchTime:0.50s; Loss: 2.1494; Accuracy: 0.4799; ValLoss: 2.6674; ValAccuracy: 0.3976\nEpoch [10/15] Time: 78.27s; BatchTime:0.50s; Loss: 2.0527; Accuracy: 0.4996; ValLoss: 2.6053; ValAccuracy: 0.4054\n\nTEST Loss: 2.6153; Accuracy: 0.4089\n\nPeak memory usage by Pytorch tensors: 4367.25 Mb\n\n"
    }
   ],
   "source": [
    "#baseline \n",
    "!python train.py --epochs=15 --batch=600 --memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Logging Timestamp 2020-03-09 14.13.51\n\nNamespace(batch=600, batch_count=1, checkpoint=3, epochs=20, memory_usage=True, model='standard', num_workers=4, opt_lr=0.001, out='saved_models/model_standard_2020-03-09 14.13.51.pth', plot_hist=False, pretrained=False, save_best=True, stop_accuracy=0.4, teacher_file='')\ndevice cuda:0\nBatch size 600\nRun training\nEpoch [1/20] Time: 43.15s; BatchTime:0.23s; Loss: 4.5243; Accuracy: 0.0869; ValLoss: 4.4051; ValAccuracy: 0.0972\nEpoch [2/20] Time: 42.04s; BatchTime:0.23s; Loss: 3.9973; Accuracy: 0.1557; ValLoss: 4.3691; ValAccuracy: 0.1222\nEpoch [3/20] Time: 41.89s; BatchTime:0.23s; Loss: 3.7493; Accuracy: 0.1921; ValLoss: 4.2940; ValAccuracy: 0.1582\nEpoch [4/20] Time: 41.53s; BatchTime:0.23s; Loss: 3.5909; Accuracy: 0.2178; ValLoss: 4.5756; ValAccuracy: 0.1489\nEpoch [5/20] Time: 41.52s; BatchTime:0.23s; Loss: 3.4685; Accuracy: 0.2395; ValLoss: 4.7812; ValAccuracy: 0.1503\nEpoch [6/20] Time: 41.94s; BatchTime:0.23s; Loss: 3.3785; Accuracy: 0.2527; ValLoss: 5.1379; ValAccuracy: 0.1525\nEpoch [7/20] Time: 41.67s; BatchTime:0.23s; Loss: 3.2955; Accuracy: 0.2654; ValLoss: 5.0964; ValAccuracy: 0.1598\nEpoch [8/20] Time: 41.75s; BatchTime:0.23s; Loss: 3.2278; Accuracy: 0.2781; ValLoss: 4.8936; ValAccuracy: 0.1781\nEpoch [9/20] Time: 41.39s; BatchTime:0.22s; Loss: 3.1777; Accuracy: 0.2850; ValLoss: 5.4895; ValAccuracy: 0.1658\nEpoch [10/20] Time: 41.59s; BatchTime:0.22s; Loss: 3.1246; Accuracy: 0.2940; ValLoss: 6.6054; ValAccuracy: 0.1751\nEpoch [11/20] Time: 41.40s; BatchTime:0.22s; Loss: 3.0855; Accuracy: 0.3022; ValLoss: 6.4206; ValAccuracy: 0.1709\nEpoch [12/20] Time: 41.66s; BatchTime:0.22s; Loss: 3.0370; Accuracy: 0.3090; ValLoss: 6.6867; ValAccuracy: 0.1731\nEpoch [13/20] Time: 41.50s; BatchTime:0.22s; Loss: 2.9972; Accuracy: 0.3159; ValLoss: 6.4923; ValAccuracy: 0.1810\nEpoch [14/20] Time: 41.72s; BatchTime:0.22s; Loss: 2.9685; Accuracy: 0.3204; ValLoss: 6.9416; ValAccuracy: 0.1682\nEpoch [15/20] Time: 42.30s; BatchTime:0.22s; Loss: 2.9300; Accuracy: 0.3284; ValLoss: 6.9432; ValAccuracy: 0.1656\nEpoch [16/20] Time: 42.06s; BatchTime:0.22s; Loss: 2.9012; Accuracy: 0.3347; ValLoss: 7.1206; ValAccuracy: 0.1682\nEpoch [17/20] Time: 42.22s; BatchTime:0.22s; Loss: 2.8802; Accuracy: 0.3372; ValLoss: 6.7625; ValAccuracy: 0.1819\nEpoch [18/20] Time: 42.71s; BatchTime:0.22s; Loss: 2.8516; Accuracy: 0.3403; ValLoss: 7.0060; ValAccuracy: 0.1737\nEpoch [19/20] Time: 42.03s; BatchTime:0.22s; Loss: 2.8283; Accuracy: 0.3461; ValLoss: 7.0273; ValAccuracy: 0.1814\nEpoch [20/20] Time: 41.33s; BatchTime:0.22s; Loss: 2.8144; Accuracy: 0.3466; ValLoss: 6.9858; ValAccuracy: 0.1790\n\nTEST Loss: 6.8134; Accuracy: 0.1793\n\nPeak memory usage by Pytorch tensors: 1250.48 Mb\n\n"
    }
   ],
   "source": [
    "#checkpoint size 3\n",
    "!python train.py --epochs=20 --batch=600 --memory_usage --checkpoint=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE! \n",
    "\n",
    "- Time for training one epoch is reduced \n",
    "- Accuracy become worst and model converges longer\n",
    "- Memory usage reduced significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mjY8LR_GQbTV"
   },
   "source": [
    "### Accumulating gradient for large batches (3 points)\n",
    "We can increase the effective batch size by simply accumulating gradients over multiple forward passes. Note that `loss.backward()` simply adds the computed gradient to `tensor.grad`, so we can call this method multiple times before actually taking an optimizer step. However, this approach might be a little tricky to combine with batch normalization. Do you see why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Logging Timestamp 2020-03-09 14.43.15\n\nNamespace(batch=100, batch_count=1, checkpoint=-1, epochs=15, memory_usage=True, model='standard', num_workers=4, opt_lr=0.001, out='saved_models/model_standard_2020-03-09 14.43.15.pth', plot_hist=False, pretrained=False, save_best=True, stop_accuracy=0.4, teacher_file='')\ndevice cuda:0\nBatch size 100\nRun training\nEpoch [1/15] Time: 78.23s; BatchTime:0.08s; Loss: 4.1771; Accuracy: 0.1248; ValLoss: 3.7837; ValAccuracy: 0.1793\nEpoch [2/15] Time: 76.60s; BatchTime:0.08s; Loss: 3.3023; Accuracy: 0.2579; ValLoss: 3.1016; ValAccuracy: 0.2986\nEpoch [3/15] Time: 77.54s; BatchTime:0.08s; Loss: 2.8911; Accuracy: 0.3337; ValLoss: 2.9480; ValAccuracy: 0.3236\nEpoch [4/15] Time: 77.73s; BatchTime:0.08s; Loss: 2.6417; Accuracy: 0.3822; ValLoss: 2.7574; ValAccuracy: 0.3714\nEpoch [5/15] Time: 78.09s; BatchTime:0.08s; Loss: 2.4533; Accuracy: 0.4184; ValLoss: 2.7410; ValAccuracy: 0.3800\nEpoch [6/15] Time: 77.59s; BatchTime:0.08s; Loss: 2.3133; Accuracy: 0.4460; ValLoss: 2.6286; ValAccuracy: 0.4032\nEpoch [7/15] Time: 76.85s; BatchTime:0.08s; Loss: 2.1909; Accuracy: 0.4712; ValLoss: 2.6830; ValAccuracy: 0.3955\nEpoch [8/15] Time: 76.32s; BatchTime:0.08s; Loss: 2.1013; Accuracy: 0.4873; ValLoss: 2.5644; ValAccuracy: 0.4174\n\nTEST Loss: 2.5695; Accuracy: 0.4181\n\nPeak memory usage by Pytorch tensors: 777.55 Mb\n\n"
    }
   ],
   "source": [
    "!python train.py --epochs=15 --batch=100 --batch_count=1 --memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Logging Timestamp 2020-03-09 14.30.25\n\nNamespace(batch=100, batch_count=10, checkpoint=-1, epochs=15, memory_usage=True, model='standard', num_workers=4, opt_lr=0.001, out='saved_models/model_standard_2020-03-09 14.30.25.pth', plot_hist=False, pretrained=False, save_best=True, stop_accuracy=0.4, teacher_file='')\ndevice cuda:0\nBatch size 100\nRun training\nEpoch [1/15] Time: 74.28s; BatchTime:0.08s; Loss: 4.4153; Accuracy: 0.0951; ValLoss: 4.0400; ValAccuracy: 0.1382\nEpoch [2/15] Time: 72.83s; BatchTime:0.08s; Loss: 3.7475; Accuracy: 0.1846; ValLoss: 3.5578; ValAccuracy: 0.2188\nEpoch [3/15] Time: 72.73s; BatchTime:0.08s; Loss: 3.3439; Accuracy: 0.2545; ValLoss: 3.2230; ValAccuracy: 0.2760\nEpoch [4/15] Time: 72.92s; BatchTime:0.08s; Loss: 3.0506; Accuracy: 0.3068; ValLoss: 3.0299; ValAccuracy: 0.3091\nEpoch [5/15] Time: 72.80s; BatchTime:0.08s; Loss: 2.8374; Accuracy: 0.3479; ValLoss: 2.8631; ValAccuracy: 0.3429\nEpoch [6/15] Time: 72.80s; BatchTime:0.08s; Loss: 2.6674; Accuracy: 0.3801; ValLoss: 2.7560; ValAccuracy: 0.3655\nEpoch [7/15] Time: 72.76s; BatchTime:0.08s; Loss: 2.5253; Accuracy: 0.4072; ValLoss: 2.6756; ValAccuracy: 0.3806\nEpoch [8/15] Time: 72.41s; BatchTime:0.08s; Loss: 2.4058; Accuracy: 0.4314; ValLoss: 2.6457; ValAccuracy: 0.3908\nEpoch [9/15] Time: 72.78s; BatchTime:0.08s; Loss: 2.3016; Accuracy: 0.4530; ValLoss: 2.5732; ValAccuracy: 0.4075\nEpoch [10/15] Time: 73.14s; BatchTime:0.08s; Loss: 2.2089; Accuracy: 0.4724; ValLoss: 2.5470; ValAccuracy: 0.4097\n\nTEST Loss: 2.5475; Accuracy: 0.4096\n\nPeak memory usage by Pytorch tensors: 777.67 Mb\n\n"
    }
   ],
   "source": [
    "!python train.py --epochs=15 --batch=100 --batch_count=10 --memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE! \n",
    "\n",
    "## Results \n",
    "- For that model (example) model converge slower from one epoch to other\n",
    "- Reduce time for one epoch - because less steps for optimizer had done \n",
    "- Memory usage stays the same "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZqxvZWH9Uxtq"
   },
   "source": [
    "**Task**. Explore the trade-off between computation time and memory usage while maintaining the same effective batch size. By effective batch size we mean the number of objects over which the loss is computed before taking a gradient step.\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "* Compare compute between accumulating gradient and gradient checkpointing with similar memory consumptions\n",
    "* Incorporate gradient accumulation into your script with optional argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3iiJZuhSUR0"
   },
   "source": [
    "## Accuracy vs compute trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fXad1svpSk8f"
   },
   "source": [
    "### Knowledge distillation (6 points)\n",
    "Suppose that we have a large network (*teacher network*) or an ensemble of networks which has a good accuracy. We can like train a much smaller network (*student network*) using the outputs of teacher networks. It turns out that the perfomance could be even better! This approach doesn't help with training speed, but can be quite beneficial when we'd like to reduce the model size for low-memory devices.\n",
    "\n",
    "* https://www.ttic.edu/dl/dark14.pdf\n",
    "* [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)\n",
    "* https://medium.com/neural-machines/knowledge-distillation-dc241d7c2322\n",
    "\n",
    "Even the completely different ([article](https://arxiv.org/abs/1711.10433)) architecture can be used in a student model, e.g. you can approximate an autoregressive model (WaveNet) by a non-autoregressive one.\n",
    "\n",
    "**Task:** \n",
    "\n",
    "1. Train good enough (teacher) network, achieve >=35% accuracy on validation set.\n",
    "\n",
    "2. Train small (student) network, achieve 20-25% accuracy, draw a plot \"training and testing errors vs train step index\"\n",
    "\n",
    "3. Distill teacher network with student network, achieve at least +1% improvement in accuracy over student network accuracy.\n",
    "\n",
    "_Please, don't cheat with early-early-early stopping while training of the student network. Make sure, it  converged._\n",
    "\n",
    "**Note**. Logits carry more information than the probabilities after softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Figure(640x480)\nFigure(640x480)\nLogging Timestamp 2020-03-09 15.00.42\n\nNamespace(batch=100, batch_count=1, checkpoint=-1, epochs=20, memory_usage=True, model='small', num_workers=4, opt_lr=0.001, out='saved_models/model_small_2020-03-09 15.00.42.pth', plot_hist=True, pretrained=False, save_best=True, stop_accuracy=0.4, teacher_file='')\ndevice cuda:0\nBatch size 100\nRun training\nEpoch [1/20] Time: 57.16s; BatchTime:0.06s; Loss: 4.6341; Accuracy: 0.0721; ValLoss: 4.3149; ValAccuracy: 0.1122\nEpoch [2/20] Time: 55.80s; BatchTime:0.06s; Loss: 4.1355; Accuracy: 0.1339; ValLoss: 4.0267; ValAccuracy: 0.1514\nEpoch [3/20] Time: 56.56s; BatchTime:0.06s; Loss: 3.8795; Accuracy: 0.1725; ValLoss: 3.8586; ValAccuracy: 0.1812\nEpoch [4/20] Time: 57.97s; BatchTime:0.06s; Loss: 3.7073; Accuracy: 0.1977; ValLoss: 3.6971; ValAccuracy: 0.2039\nEpoch [5/20] Time: 57.78s; BatchTime:0.06s; Loss: 3.5807; Accuracy: 0.2215; ValLoss: 3.6029; ValAccuracy: 0.2208\nEpoch [6/20] Time: 57.62s; BatchTime:0.06s; Loss: 3.4820; Accuracy: 0.2365; ValLoss: 3.5658; ValAccuracy: 0.2242\nEpoch [7/20] Time: 57.71s; BatchTime:0.06s; Loss: 3.4068; Accuracy: 0.2470; ValLoss: 3.4884; ValAccuracy: 0.2387\nEpoch [8/20] Time: 57.78s; BatchTime:0.06s; Loss: 3.3487; Accuracy: 0.2597; ValLoss: 3.4351; ValAccuracy: 0.2482\nEpoch [9/20] Time: 57.59s; BatchTime:0.06s; Loss: 3.2944; Accuracy: 0.2661; ValLoss: 3.5105; ValAccuracy: 0.2419\nEpoch [10/20] Time: 57.69s; BatchTime:0.06s; Loss: 3.2550; Accuracy: 0.2742; ValLoss: 3.3671; ValAccuracy: 0.2603\nEpoch [11/20] Time: 57.69s; BatchTime:0.06s; Loss: 3.2222; Accuracy: 0.2803; ValLoss: 3.3745; ValAccuracy: 0.2629\nEpoch [12/20] Time: 57.74s; BatchTime:0.06s; Loss: 3.1870; Accuracy: 0.2869; ValLoss: 3.3707; ValAccuracy: 0.2590\nEpoch [13/20] Time: 57.82s; BatchTime:0.06s; Loss: 3.1633; Accuracy: 0.2907; ValLoss: 3.3416; ValAccuracy: 0.2628\nEpoch [14/20] Time: 57.64s; BatchTime:0.06s; Loss: 3.1356; Accuracy: 0.2950; ValLoss: 3.3374; ValAccuracy: 0.2695\nEpoch [15/20] Time: 58.07s; BatchTime:0.06s; Loss: 3.1146; Accuracy: 0.3007; ValLoss: 3.3036; ValAccuracy: 0.2693\nEpoch [16/20] Time: 57.45s; BatchTime:0.06s; Loss: 3.0930; Accuracy: 0.3015; ValLoss: 3.2821; ValAccuracy: 0.2757\nEpoch [17/20] Time: 57.43s; BatchTime:0.06s; Loss: 3.0757; Accuracy: 0.3060; ValLoss: 3.2594; ValAccuracy: 0.2790\nEpoch [18/20] Time: 58.07s; BatchTime:0.06s; Loss: 3.0623; Accuracy: 0.3079; ValLoss: 3.2696; ValAccuracy: 0.2782\nEpoch [19/20] Time: 57.10s; BatchTime:0.06s; Loss: 3.0434; Accuracy: 0.3108; ValLoss: 3.2553; ValAccuracy: 0.2772\nEpoch [20/20] Time: 57.40s; BatchTime:0.06s; Loss: 3.0308; Accuracy: 0.3125; ValLoss: 3.2551; ValAccuracy: 0.2868\n\nTEST Loss: 3.2535; Accuracy: 0.2847\n\nPeak memory usage by Pytorch tensors: 628.61 Mb\n\n"
    }
   ],
   "source": [
    "!python train.py --epochs=20 --batch=100 --model=small --memory_usage --plot_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Figure(640x480)\nFigure(640x480)\nLogging Timestamp 2020-03-09 15.20.41\n\nNamespace(batch=100, batch_count=1, checkpoint=-1, epochs=20, memory_usage=True, model='small', num_workers=4, opt_lr=0.001, out='saved_models/model_small_2020-03-09 15.20.41.pth', plot_hist=True, pretrained=False, save_best=True, stop_accuracy=0.4, teacher_file='')\ndevice cuda:0\nBatch size 100\nRun training\nEpoch [1/20] Time: 42.56s; BatchTime:0.04s; Loss: 4.7862; Accuracy: 0.0562; ValLoss: 4.5141; ValAccuracy: 0.0783\nEpoch [2/20] Time: 41.50s; BatchTime:0.04s; Loss: 4.3325; Accuracy: 0.1060; ValLoss: 4.1929; ValAccuracy: 0.1203\nEpoch [3/20] Time: 41.42s; BatchTime:0.04s; Loss: 4.0925; Accuracy: 0.1391; ValLoss: 4.0062; ValAccuracy: 0.1541\nEpoch [4/20] Time: 41.13s; BatchTime:0.04s; Loss: 3.9400; Accuracy: 0.1627; ValLoss: 3.9197; ValAccuracy: 0.1665\nEpoch [5/20] Time: 41.26s; BatchTime:0.04s; Loss: 3.8238; Accuracy: 0.1823; ValLoss: 3.8094; ValAccuracy: 0.1840\nEpoch [6/20] Time: 41.43s; BatchTime:0.04s; Loss: 3.7382; Accuracy: 0.1937; ValLoss: 3.7137; ValAccuracy: 0.1991\nEpoch [7/20] Time: 41.18s; BatchTime:0.04s; Loss: 3.6750; Accuracy: 0.2041; ValLoss: 3.7139; ValAccuracy: 0.1947\nEpoch [8/20] Time: 41.25s; BatchTime:0.04s; Loss: 3.6221; Accuracy: 0.2122; ValLoss: 3.6612; ValAccuracy: 0.2071\nEpoch [9/20] Time: 41.12s; BatchTime:0.04s; Loss: 3.5804; Accuracy: 0.2208; ValLoss: 3.7222; ValAccuracy: 0.1962\nEpoch [10/20] Time: 41.30s; BatchTime:0.04s; Loss: 3.5430; Accuracy: 0.2252; ValLoss: 3.6422; ValAccuracy: 0.2152\nEpoch [11/20] Time: 40.91s; BatchTime:0.04s; Loss: 3.5139; Accuracy: 0.2296; ValLoss: 3.5753; ValAccuracy: 0.2222\nEpoch [12/20] Time: 41.03s; BatchTime:0.04s; Loss: 3.4901; Accuracy: 0.2334; ValLoss: 3.5325; ValAccuracy: 0.2292\nEpoch [13/20] Time: 41.32s; BatchTime:0.04s; Loss: 3.4652; Accuracy: 0.2378; ValLoss: 3.5665; ValAccuracy: 0.2250\nEpoch [14/20] Time: 41.10s; BatchTime:0.04s; Loss: 3.4433; Accuracy: 0.2415; ValLoss: 3.5503; ValAccuracy: 0.2257\nEpoch [15/20] Time: 41.27s; BatchTime:0.04s; Loss: 3.4255; Accuracy: 0.2428; ValLoss: 3.5455; ValAccuracy: 0.2306\nEpoch [16/20] Time: 41.10s; BatchTime:0.04s; Loss: 3.4098; Accuracy: 0.2468; ValLoss: 3.5325; ValAccuracy: 0.2306\nEpoch [17/20] Time: 41.50s; BatchTime:0.04s; Loss: 3.3933; Accuracy: 0.2504; ValLoss: 3.4604; ValAccuracy: 0.2415\nEpoch [18/20] Time: 39.04s; BatchTime:0.04s; Loss: 3.3809; Accuracy: 0.2513; ValLoss: 3.4919; ValAccuracy: 0.2364\nEpoch [19/20] Time: 38.00s; BatchTime:0.04s; Loss: 3.3672; Accuracy: 0.2555; ValLoss: 3.4452; ValAccuracy: 0.2453\nEpoch [20/20] Time: 38.62s; BatchTime:0.04s; Loss: 3.3554; Accuracy: 0.2576; ValLoss: 3.4450; ValAccuracy: 0.2459\n\nTEST Loss: 3.4424; Accuracy: 0.2465\n\nPeak memory usage by Pytorch tensors: 316.53 Mb\n\n"
    }
   ],
   "source": [
    "!python train.py --epochs=20 --batch=100 --model=small --memory_usage --plot_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Figure(640x480)\nFigure(640x480)\nLogging Timestamp 2020-03-09 15.37.01\n\nNamespace(batch=100, batch_count=1, checkpoint=-1, epochs=20, memory_usage=True, model='small', num_workers=4, opt_lr=0.001, out='saved_models/model_small_2020-03-09 15.37.01.pth', plot_hist=True, pretrained=False, save_best=True, stop_accuracy=0.4, teacher_file='')\ndevice cuda:0\nBatch size 100\nRun training\nEpoch [1/20] Time: 32.59s; BatchTime:0.03s; Loss: 4.9253; Accuracy: 0.0395; ValLoss: 4.6935; ValAccuracy: 0.0616\nEpoch [2/20] Time: 32.23s; BatchTime:0.03s; Loss: 4.5649; Accuracy: 0.0754; ValLoss: 4.4551; ValAccuracy: 0.0924\nEpoch [3/20] Time: 31.81s; BatchTime:0.03s; Loss: 4.3590; Accuracy: 0.1021; ValLoss: 4.2836; ValAccuracy: 0.1113\nEpoch [4/20] Time: 31.42s; BatchTime:0.03s; Loss: 4.2178; Accuracy: 0.1208; ValLoss: 4.1876; ValAccuracy: 0.1226\nEpoch [5/20] Time: 31.50s; BatchTime:0.03s; Loss: 4.1164; Accuracy: 0.1357; ValLoss: 4.1061; ValAccuracy: 0.1328\nEpoch [6/20] Time: 31.52s; BatchTime:0.03s; Loss: 4.0440; Accuracy: 0.1448; ValLoss: 4.0486; ValAccuracy: 0.1432\nEpoch [7/20] Time: 31.44s; BatchTime:0.03s; Loss: 3.9897; Accuracy: 0.1520; ValLoss: 3.9851; ValAccuracy: 0.1519\nEpoch [8/20] Time: 31.50s; BatchTime:0.03s; Loss: 3.9390; Accuracy: 0.1590; ValLoss: 3.9126; ValAccuracy: 0.1634\nEpoch [9/20] Time: 31.44s; BatchTime:0.03s; Loss: 3.9028; Accuracy: 0.1662; ValLoss: 3.9528; ValAccuracy: 0.1558\nEpoch [10/20] Time: 31.66s; BatchTime:0.03s; Loss: 3.8683; Accuracy: 0.1694; ValLoss: 3.8779; ValAccuracy: 0.1693\nEpoch [11/20] Time: 31.62s; BatchTime:0.03s; Loss: 3.8434; Accuracy: 0.1737; ValLoss: 3.8536; ValAccuracy: 0.1750\nEpoch [12/20] Time: 30.97s; BatchTime:0.03s; Loss: 3.8191; Accuracy: 0.1778; ValLoss: 3.8430; ValAccuracy: 0.1756\nEpoch [13/20] Time: 31.02s; BatchTime:0.03s; Loss: 3.7988; Accuracy: 0.1806; ValLoss: 3.8307; ValAccuracy: 0.1745\nEpoch [14/20] Time: 31.19s; BatchTime:0.03s; Loss: 3.7830; Accuracy: 0.1812; ValLoss: 3.7953; ValAccuracy: 0.1836\nEpoch [15/20] Time: 31.47s; BatchTime:0.03s; Loss: 3.7675; Accuracy: 0.1862; ValLoss: 3.7792; ValAccuracy: 0.1839\nEpoch [16/20] Time: 31.41s; BatchTime:0.03s; Loss: 3.7507; Accuracy: 0.1875; ValLoss: 3.7747; ValAccuracy: 0.1866\nEpoch [17/20] Time: 31.40s; BatchTime:0.03s; Loss: 3.7407; Accuracy: 0.1893; ValLoss: 3.7716; ValAccuracy: 0.1861\nEpoch [18/20] Time: 30.87s; BatchTime:0.03s; Loss: 3.7282; Accuracy: 0.1917; ValLoss: 3.7526; ValAccuracy: 0.1846\nEpoch [19/20] Time: 31.37s; BatchTime:0.03s; Loss: 3.7176; Accuracy: 0.1930; ValLoss: 3.7455; ValAccuracy: 0.1888\nEpoch [20/20] Time: 31.63s; BatchTime:0.03s; Loss: 3.7112; Accuracy: 0.1943; ValLoss: 3.7377; ValAccuracy: 0.1899\n\nTEST Loss: 3.7392; Accuracy: 0.1937\n\nPeak memory usage by Pytorch tensors: 161.09 Mb\n\n"
    }
   ],
   "source": [
    "!python train.py --epochs=20 --batch=100 --model=small --memory_usage --plot_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillModel(nn.Module):\n",
    "    def __init__(self, teacher, student, tau=1):\n",
    "        super(DistillModel, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "        self.tau = tau\n",
    "\n",
    "        for param in self.teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, input):\n",
    "        teacher_out = self.teacher(input)\n",
    "        student_out = self.student(input)\n",
    "\n",
    "        return student_out, teacher_out.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, tau=20, alpha=0.7):\n",
    "        super(DistillationLoss, self).__init__()\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        self.KLDiv_criterion = nn.KLDivLoss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        student_out, teacher_out = outputs\n",
    "\n",
    "        KLDiv_loss = self.KLDiv_criterion(nn.LogSoftmax()(student_out/self.tau),\n",
    "                                         nn.Softmax()(teacher_out/self.tau)) \n",
    "\n",
    "        cross_entropy_loss = self.cross_entropy(student_out, targets)\n",
    "\n",
    "        return self.alpha * KLDiv_loss # + (1 - self.alpha) * cross_entropy_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationAccuracy(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistillationAccuracy, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        student, teacher = outputs\n",
    "        _, preds = torch.max(student, 1)\n",
    "        return torch.mean((preds == targets).double())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Figure(640x480)\nFigure(640x480)\nLogging Timestamp 2020-03-09 16.01.40\n\nNamespace(batch=100, batch_count=1, checkpoint=-1, epochs=30, memory_usage=True, model='distill', num_workers=4, opt_lr=0.001, out='saved_models/model_distill_2020-03-09 16.01.40.pth', plot_hist=True, pretrained=False, save_best=True, stop_accuracy=0.4, teacher_file='model_standard.pth')\ndevice cuda:0\nBatch size 100\nRun training\nEpoch [1/30] Time: 52.88s; BatchTime:0.05s; Loss: 1.4756; Accuracy: 0.0405; ValLoss: 1.4044; ValAccuracy: 0.0674\nEpoch [2/30] Time: 52.15s; BatchTime:0.05s; Loss: 1.3632; Accuracy: 0.0794; ValLoss: 1.3229; ValAccuracy: 0.0973\nEpoch [3/30] Time: 52.24s; BatchTime:0.05s; Loss: 1.3010; Accuracy: 0.1042; ValLoss: 1.2837; ValAccuracy: 0.1153\nEpoch [4/30] Time: 52.15s; BatchTime:0.05s; Loss: 1.2620; Accuracy: 0.1215; ValLoss: 1.2558; ValAccuracy: 0.1266\nEpoch [5/30] Time: 52.23s; BatchTime:0.05s; Loss: 1.2357; Accuracy: 0.1341; ValLoss: 1.2311; ValAccuracy: 0.1406\nEpoch [6/30] Time: 52.32s; BatchTime:0.05s; Loss: 1.2158; Accuracy: 0.1436; ValLoss: 1.2162; ValAccuracy: 0.1440\nEpoch [7/30] Time: 52.23s; BatchTime:0.05s; Loss: 1.2006; Accuracy: 0.1511; ValLoss: 1.2003; ValAccuracy: 0.1559\nEpoch [8/30] Time: 52.19s; BatchTime:0.05s; Loss: 1.1864; Accuracy: 0.1575; ValLoss: 1.1959; ValAccuracy: 0.1529\nEpoch [9/30] Time: 52.13s; BatchTime:0.05s; Loss: 1.1753; Accuracy: 0.1640; ValLoss: 1.2029; ValAccuracy: 0.1504\nEpoch [10/30] Time: 52.05s; BatchTime:0.05s; Loss: 1.1643; Accuracy: 0.1691; ValLoss: 1.1729; ValAccuracy: 0.1651\nEpoch [11/30] Time: 52.18s; BatchTime:0.05s; Loss: 1.1551; Accuracy: 0.1726; ValLoss: 1.1684; ValAccuracy: 0.1716\nEpoch [12/30] Time: 52.09s; BatchTime:0.05s; Loss: 1.1479; Accuracy: 0.1764; ValLoss: 1.1582; ValAccuracy: 0.1719\nEpoch [13/30] Time: 52.24s; BatchTime:0.05s; Loss: 1.1415; Accuracy: 0.1810; ValLoss: 1.1555; ValAccuracy: 0.1800\nEpoch [14/30] Time: 52.19s; BatchTime:0.05s; Loss: 1.1358; Accuracy: 0.1834; ValLoss: 1.1584; ValAccuracy: 0.1754\nEpoch [15/30] Time: 52.10s; BatchTime:0.05s; Loss: 1.1299; Accuracy: 0.1859; ValLoss: 1.1523; ValAccuracy: 0.1765\nEpoch [16/30] Time: 52.01s; BatchTime:0.05s; Loss: 1.1255; Accuracy: 0.1876; ValLoss: 1.1479; ValAccuracy: 0.1786\nEpoch [17/30] Time: 52.08s; BatchTime:0.05s; Loss: 1.1216; Accuracy: 0.1909; ValLoss: 1.1458; ValAccuracy: 0.1800\nEpoch [18/30] Time: 52.08s; BatchTime:0.05s; Loss: 1.1187; Accuracy: 0.1926; ValLoss: 1.1381; ValAccuracy: 0.1855\nEpoch [19/30] Time: 52.14s; BatchTime:0.05s; Loss: 1.1144; Accuracy: 0.1946; ValLoss: 1.1302; ValAccuracy: 0.1881\nEpoch [20/30] Time: 52.16s; BatchTime:0.05s; Loss: 1.1124; Accuracy: 0.1943; ValLoss: 1.1313; ValAccuracy: 0.1870\nEpoch [21/30] Time: 52.02s; BatchTime:0.05s; Loss: 1.1093; Accuracy: 0.1974; ValLoss: 1.1266; ValAccuracy: 0.1870\nEpoch [22/30] Time: 52.00s; BatchTime:0.05s; Loss: 1.1066; Accuracy: 0.1981; ValLoss: 1.1451; ValAccuracy: 0.1836\nEpoch [23/30] Time: 52.08s; BatchTime:0.05s; Loss: 1.1042; Accuracy: 0.2016; ValLoss: 1.1198; ValAccuracy: 0.1922\nEpoch [24/30] Time: 52.09s; BatchTime:0.05s; Loss: 1.1019; Accuracy: 0.2006; ValLoss: 1.1206; ValAccuracy: 0.1927\nEpoch [25/30] Time: 52.23s; BatchTime:0.05s; Loss: 1.0999; Accuracy: 0.2009; ValLoss: 1.1331; ValAccuracy: 0.1876\nEpoch [26/30] Time: 51.97s; BatchTime:0.05s; Loss: 1.0977; Accuracy: 0.2026; ValLoss: 1.1193; ValAccuracy: 0.1942\nEpoch [27/30] Time: 52.24s; BatchTime:0.05s; Loss: 1.0957; Accuracy: 0.2041; ValLoss: 1.1146; ValAccuracy: 0.1972\nEpoch [28/30] Time: 52.11s; BatchTime:0.05s; Loss: 1.0939; Accuracy: 0.2038; ValLoss: 1.1091; ValAccuracy: 0.1991\nEpoch [29/30] Time: 52.14s; BatchTime:0.05s; Loss: 1.0913; Accuracy: 0.2068; ValLoss: 1.1189; ValAccuracy: 0.1954\nEpoch [30/30] Time: 52.05s; BatchTime:0.05s; Loss: 1.0906; Accuracy: 0.2072; ValLoss: 1.1101; ValAccuracy: 0.1977\n\nTEST Loss: 1.1081; Accuracy: 0.1996\n\nPeak memory usage by Pytorch tensors: 211.36 Mb\n\n"
    }
   ],
   "source": [
    "!python train.py --epochs=30 --batch=100 --model=distill --memory_usage --plot_hist --teacher_file=model_standard.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL;DR. Moar techniques on accuracy vs time trade-off (just for your information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WOWhqMJSboR"
   },
   "source": [
    "### Tensor type size\n",
    "\n",
    "One of the hyperparameter affecting memory consumption is the precision (e.g. floating point number). The most popular choice is 32 bit however with several hacks* 16 bit arithmetics can save you approximately half of the memory without considerable loss of perfomance. This is called mixed precision training.\n",
    "\n",
    "*https://arxiv.org/pdf/1710.03740.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xAEF9aJc-43"
   },
   "source": [
    "### Quantization\n",
    "\n",
    "We can actually move further and use even lower precision like 8-bit integers:\n",
    "\n",
    "* https://heartbeat.fritz.ai/8-bit-quantization-and-tensorflow-lite-speeding-up-mobile-inference-with-low-precision-a882dfcafbbd\n",
    "* https://nervanasystems.github.io/distiller/quantization/\n",
    "* https://arxiv.org/abs/1712.05877"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "\n",
    "The idea of pruning is to remove unnecessary (in terms of loss) weights. It can be measured in different ways: for example, by the norm of the weights (similar to L1 feature selection), by the magnitude of the activation or via Taylor expansion*.\n",
    "\n",
    "One iteration of pruning consists of two steps:\n",
    "\n",
    "1) Rank weights with some importance measure and remove the least important\n",
    "\n",
    "2) Fine-tune the model\n",
    "\n",
    "This approach is a bit computationally heavy but can lead to drastic (up to 150x) decrease of memory to store the weights. Moreover if you make use of structure in layers you can decrease also compute. For example, the whole convolutional filters can be removed.\n",
    "\n",
    "*https://arxiv.org/pdf/1611.06440.pdf"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "homework_optimization.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda9bee05429bb2459f8df85aaadcba0215"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}